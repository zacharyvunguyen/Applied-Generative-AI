{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"/Users/zacharynguyen/Documents/GitHub/2024/Applied-Generative-AI/IAM/zacharynguyen-genai-656c475b142a.json\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_APPLICATION_CREDENTIALS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:41.991801Z",
     "start_time": "2024-03-27T15:30:38.475672Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:42.240452Z",
     "start_time": "2024-03-27T15:30:41.984200Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'zacharynguyen-genai' # replace with project ID\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs and API Enablement\n",
    "\n",
    "The clients packages may need installing in this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:42.394249Z",
     "start_time": "2024-03-27T15:30:42.208832Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Enablement"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!gcloud auth login"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:42.569077Z",
     "start_time": "2024-03-27T15:30:42.363874Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:42.679556Z",
     "start_time": "2024-03-27T15:30:42.563792Z"
    }
   },
   "outputs": [],
   "source": [
    "#!gcloud services enable aiplatform.googleapis.com\n",
    "#!gcloud services enable documentai.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "appt8-yVRtJ1"
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63mx2EozRxFP"
   },
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2124,
     "status": "ok",
     "timestamp": 1683726390544,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "xzcoXjM5Rky5",
    "outputId": "b3bdcbc1-70d5-472e-aea2-42c74a42efde",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:47.333775Z",
     "start_time": "2024-03-27T15:30:42.663524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'zacharynguyen-genai'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "project[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683726390712,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "IxWrFtqYMfku",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:47.366590Z",
     "start_time": "2024-03-27T15:30:47.297471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project and Experiment Configuration\n",
    "PROJECT_ID = 'zacharynguyen-genai'\n",
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'mlb-rules'\n",
    "SERIES = 'applied-genai-v3'\n",
    "\n",
    "# Storage Configuration\n",
    "# Determines where to save results (GCS, BQ, or both)\n",
    "SAVE_IN = 'ALL'\n",
    "# Specifies the source from which to retrieve previous results, if available\n",
    "RETRIEVE_FROM = 'GCS'\n",
    "\n",
    "# Google Cloud Storage (GCS) Bucket Name\n",
    "GCS_BUCKET = PROJECT_ID  # Using the project ID as the bucket name for simplicity\n",
    "\n",
    "# BigQuery (BQ) Configuration\n",
    "# Uses project ID for BQ project. Dataset and table names are derived from the experiment details\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES.replace('-', '_')  # Replacing dashes with underscores for BQ dataset naming\n",
    "BQ_TABLE = EXPERIMENT\n",
    "BQ_REGION = REGION[:2]  # Extracts the first two characters from the REGION for BQ region code\n",
    "\n",
    "# Document Source Configuration\n",
    "# A list of source document URLs. Supports http:// or gs:// protocols\n",
    "source_documents = [\n",
    "    'https://img.mlbstatic.com/mlb-images/image/upload/mlb/wqn5ah4c3qtivwx3jatm.pdf'\n",
    "]\n",
    "\n",
    "# Operational Flags\n",
    "# Determines whether to use data from a previous run or force a new run by deleting existing data\n",
    "USE_PRIOR_RUN = True\n",
    "\n",
    "# Initial Question for Processing\n",
    "question = \"How is baseball played?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processing Workflow Configuration\n",
      "\n",
      "Project and Experiment Details:\n",
      "  Project ID: zacharynguyen-genai\n",
      "  Region: us-central1\n",
      "  Experiment: mlb-rules\n",
      "  Series: applied-genai-v3\n",
      "\n",
      "Storage Configuration:\n",
      "  Results Saving Option: ALL\n",
      "  Results Retrieval Source: GCS\n",
      "  GCS Bucket: zacharynguyen-genai\n",
      "\n",
      "BigQuery Configuration:\n",
      "  BQ Project: zacharynguyen-genai\n",
      "  BQ Dataset: applied_genai_v3\n",
      "  BQ Table: mlb-rules\n",
      "  BQ Region: us\n",
      "\n",
      "Document Source Details:\n",
      "  Document 1: https://img.mlbstatic.com/mlb-images/image/upload/mlb/wqn5ah4c3qtivwx3jatm.pdf\n",
      "\n",
      "Operational Flags:\n",
      "  Use Prior Run: Yes\n",
      "\n",
      "Initial Question for Processing:\n",
      "  Question: \"How is baseball played?\"\n"
     ]
    }
   ],
   "source": [
    "def print_configuration():\n",
    "    print(\"Document Processing Workflow Configuration\\n\")\n",
    "    print(f\"Project and Experiment Details:\")\n",
    "    print(f\"  Project ID: {PROJECT_ID}\")\n",
    "    print(f\"  Region: {REGION}\")\n",
    "    print(f\"  Experiment: {EXPERIMENT}\")\n",
    "    print(f\"  Series: {SERIES}\\n\")\n",
    "    \n",
    "    print(f\"Storage Configuration:\")\n",
    "    print(f\"  Results Saving Option: {SAVE_IN}\")\n",
    "    print(f\"  Results Retrieval Source: {RETRIEVE_FROM}\")\n",
    "    print(f\"  GCS Bucket: {GCS_BUCKET}\\n\")\n",
    "    \n",
    "    print(f\"BigQuery Configuration:\")\n",
    "    print(f\"  BQ Project: {BQ_PROJECT}\")\n",
    "    print(f\"  BQ Dataset: {BQ_DATASET}\")\n",
    "    print(f\"  BQ Table: {BQ_TABLE}\")\n",
    "    print(f\"  BQ Region: {BQ_REGION}\\n\")\n",
    "    \n",
    "    print(f\"Document Source Details:\")\n",
    "    if source_documents:\n",
    "        for i, doc in enumerate(source_documents, start=1):\n",
    "            print(f\"  Document {i}: {doc}\")\n",
    "    else:\n",
    "        print(\"  No source documents specified.\\n\")\n",
    "    \n",
    "    print(f\"\\nOperational Flags:\")\n",
    "    print(f\"  Use Prior Run: {'Yes' if USE_PRIOR_RUN else 'No'}\")\n",
    "    \n",
    "    print(f\"\\nInitial Question for Processing:\")\n",
    "    print(f\"  Question: \\\"{question}\\\"\")\n",
    "\n",
    "# Call the function to print the configuration\n",
    "print_configuration()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:47.401998Z",
     "start_time": "2024-03-27T15:30:47.308277Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuajVwCiO6Yg"
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 17761,
     "status": "ok",
     "timestamp": 1683726409304,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "LVC7zzSLRk2C",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:47.637121Z",
     "start_time": "2024-03-27T15:30:47.323899Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "import PyPDF2\n",
    "import IPython\n",
    "import PIL, PIL.ImageFont, PIL.Image, PIL.ImageDraw\n",
    "import shapely\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import vertexai.language_models # PaLM and Codey Models\n",
    "import vertexai.generative_models # for Gemini Models\n",
    "from google.cloud import documentai\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.api_core import retry"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create GCS Bucket"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_gcs_bucket(project_id, bucket_name, region):\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    \n",
    "    # Check if the bucket already exists\n",
    "    try:\n",
    "        existing_bucket = storage_client.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {existing_bucket.name} already exists.\")\n",
    "        return existing_bucket\n",
    "    except NotFound:\n",
    "        # If the bucket does not exist, proceed to create it\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        new_bucket = storage_client.create_bucket(bucket, location=region)\n",
    "        print(f\"Bucket {new_bucket.name} created.\")\n",
    "        return new_bucket"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:47.687181Z",
     "start_time": "2024-03-27T15:30:47.335208Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket zacharynguyen-genai already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Bucket: zacharynguyen-genai>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create GCS Bucket\n",
    "create_gcs_bucket(PROJECT_ID, GCS_BUCKET, REGION)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:48.059280Z",
     "start_time": "2024-03-27T15:30:47.341103Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataset and table in BigQuery"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_bq_dataset_and_table(project_id, dataset_id, table_id, region):\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Attempt to get the dataset, create if not exists\n",
    "    dataset_ref = bigquery.DatasetReference(project_id, dataset_id)\n",
    "    try:\n",
    "        dataset = bq_client.get_dataset(dataset_ref)  # Make an API request.\n",
    "        print(f\"Dataset {dataset.dataset_id} already exists.\")\n",
    "    except NotFound:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = region\n",
    "        dataset = bq_client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset {dataset.dataset_id} created.\")\n",
    "\n",
    "    # Attempt to get the table, create if not exists\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    try:\n",
    "        table = bq_client.get_table(table_ref)  # Make an API request.\n",
    "        print(f\"Table {table.table_id} already exists.\")\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"example_field\", \"STRING\", mode=\"NULLABLE\")\n",
    "        ]\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table = bq_client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table {table.table_id} created.\")\n",
    "\n",
    "    return table\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:48.078316Z",
     "start_time": "2024-03-27T15:30:47.774581Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset applied_genai_v3 already exists.\n",
      "Table mlb-rules already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Table(TableReference(DatasetReference('zacharynguyen-genai', 'applied_genai_v3'), 'mlb-rules'))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create BigQuery Dataset and Table\n",
    "create_bq_dataset_and_table(PROJECT_ID, BQ_DATASET, BQ_TABLE, BQ_REGION)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:30:48.823314Z",
     "start_time": "2024-03-27T15:30:47.783587Z"
    }
   },
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyAVFG9TO9H-"
   },
   "source": [
    "Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1683726409306,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "L0RPE13LOZce",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:36.472941Z",
     "start_time": "2024-03-27T15:34:36.339876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients for Vertex AI, Document AI, BigQuery, and GCS have been initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import documentai, bigquery, storage\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI with specified project and location\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Derive the Document AI service location from the REGION variable\n",
    "LOCATION = REGION.split('-')[0]\n",
    "docai_api_endpoint = f\"{location}-documentai.googleapis.com\"\n",
    "\n",
    "# Initialize the Document AI clients for both synchronous and asynchronous operations\n",
    "docai_client = documentai.DocumentProcessorServiceClient(client_options={\"api_endpoint\": docai_api_endpoint})\n",
    "docai_async_client = documentai.DocumentProcessorServiceAsyncClient(client_options={\"api_endpoint\": docai_api_endpoint})\n",
    "\n",
    "# Initialize the BigQuery client for data analytics operations\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize the Google Cloud Storage client and retrieve the specified bucket\n",
    "gcs = storage.Client(project=PROJECT_ID)\n",
    "bucket = gcs.bucket(GCS_BUCKET)\n",
    "\n",
    "# Confirmation of clients initialization\n",
    "print(\"Clients for Vertex AI, Document AI, BigQuery, and GCS have been initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF_zXB00TMao"
   },
   "source": [
    "---\n",
    "## Vertex LLM Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1683726409314,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "dn2FH-pETUPf",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:44.085916Z",
     "start_time": "2024-03-27T15:34:43.054683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI models initialized successfully:\n",
      "- Gemini Text Model: gemini_text\n",
      "- PaLM Text Embedding Model: textembed_model\n",
      "- PaLM Text Generation Models: text_model_b, text_model_b32, text_model_u\n"
     ]
    }
   ],
   "source": [
    "from vertexai import generative_models, language_models\n",
    "\n",
    "# Initializing Gemini Models\n",
    "# Gemini Text Model for advanced text generation and understanding\n",
    "gemini_text = generative_models.GenerativeModel(\"gemini-1.0-pro\")\n",
    "\n",
    "# Initializing PaLM Models\n",
    "# Text Embedding Model for dense vector representations of text\n",
    "textembed_model = language_models.TextEmbeddingModel.from_pretrained('textembedding-gecko')\n",
    "\n",
    "# Text Generation Models for various text generation tasks\n",
    "# Standard Bison Model\n",
    "text_model_b = language_models.TextGenerationModel.from_pretrained('text-bison')\n",
    "\n",
    "# Extended Bison Model supporting up to 32k tokens\n",
    "text_model_b32 = language_models.TextGenerationModel.from_pretrained('text-bison-32k')\n",
    "\n",
    "# Unicorn Model for a broad range of text generation applications\n",
    "text_model_u = language_models.TextGenerationModel.from_pretrained('text-unicorn')\n",
    "\n",
    "# Confirmation of model initialization\n",
    "print(\"Vertex AI models initialized successfully:\")\n",
    "print(\"- Gemini Text Model: gemini_text\")\n",
    "print(\"- PaLM Text Embedding Model: textembed_model\")\n",
    "print(\"- PaLM Text Generation Models: text_model_b, text_model_b32, text_model_u\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:44.622803Z",
     "start_time": "2024-03-27T15:34:44.482277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'How is baseball played?'"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:45.784014Z",
     "start_time": "2024-03-27T15:34:45.452406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[0.02588232420384884,\n -0.03225235641002655,\n -0.01948062889277935,\n 0.01719886064529419,\n -0.002375800861045718]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textembed_model.get_embeddings([question])[0].values[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEITqYF1QNkP"
   },
   "source": [
    "### Generation: PaLM `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'How is baseball played?'"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:46.326779Z",
     "start_time": "2024-03-27T15:34:46.170454Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1683726411751,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "ippLc2_eYeQH",
    "outputId": "c09300ed-9e44-40a7-f3d0-f4a95515021c",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:48.145623Z",
     "start_time": "2024-03-27T15:34:46.578615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": " **Objective:**\nThe objective of baseball is to score runs by hitting the ball and advancing around the bases. The team with the most runs at the end of the game wins.\n\n**Gameplay:**\n- A baseball game is played between two teams, each with nine players on the field.\n- The game is divided into nine innings, each consisting of two halves: the top of the inning when the visiting team bats, and the bottom of the inning when the home team bats.\n- Each team takes turns batting and fielding. The batting team tries to hit the ball into fair territory (between the foul lines) and advance around"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = text_model_b.predict(question)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:48.175357Z",
     "start_time": "2024-03-27T15:34:48.110804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'Derogatory': 0.1, 'Insult': 0.1, 'Sexual': 0.1}"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.safety_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:52.073007Z",
     "start_time": "2024-03-27T15:34:48.120108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": " **Objective:**\nThe objective of baseball is to score runs by hitting the ball and advancing around the bases. The team with the most runs at the end of the game wins.\n\n**Gameplay:**\n- A baseball game is played between two teams, each with nine players on the field.\n- The game is divided into nine innings, each consisting of two halves: the top of the inning when the visiting team bats, and the bottom of the inning when the home team bats.\n- Each team takes turns batting and fielding. The batting team tries to hit the ball into fair territory (between the foul lines) and advance around the bases. The fielding team tries to prevent the batters from reaching base and to get them out.\n\n**Basic Rules:**\n- A player can advance around the bases by hitting the ball and reaching first base, or by being awarded a walk (four balls outside the strike zone).\n- A player can also advance around the bases by stealing a base (running to the next base while the pitcher is delivering the ball).\n- A player scores a run when they touch home plate after advancing around all the bases.\n- The pitcher throws the ball from the pitcher's mound to the catcher, who is positioned behind home plate. The batter stands at home plate and tries to hit the ball with a bat.\n- If the batter hits the ball into fair territory, they can run to first base. If they reach first base safely, they can try to advance to second base, third base, and home plate.\n- The fielders try to catch the ball before it hits the ground or to throw the ball to a fielder who can catch it. If a fielder catches the ball before it hits the ground, the batter is out.\n- If the batter hits the ball out of the park (over the fence), they hit a home run and score a run.\n- The game ends after nine innings, or after extra innings if the score is tied. The team with the most runs at the end of the game wins.\n\n**Scoring:**\n- A team scores a run when a player touches home plate after advancing around all the bases.\n- A player can score a run by hitting a home run, by being driven in (batted in) by another player, or by advancing around the bases on an error by the fielding team.\n\n**Outs:**\n- A batter is out if they:\n  -"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = text_model_b.predict(question, max_output_tokens = 500)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:52.128508Z",
     "start_time": "2024-03-27T15:34:52.027936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": " **Objective:**\nThe objective of baseball is to score runs by hitting the ball and advancing around the bases. The team with the most runs at the end of the game wins.\n\n**Gameplay:**\n- A baseball game is played between two teams, each with nine players on the field.\n- The game is divided into nine innings, each consisting of two halves: the top of the inning when the visiting team bats, and the bottom of the inning when the home team bats.\n- Each team takes turns batting and fielding. The batting team tries to hit the ball into fair territory (between the foul lines) and advance around the bases. The fielding team tries to prevent the batters from reaching base and to get them out.\n\n**Basic Rules:**\n- A player can advance around the bases by hitting the ball and reaching first base, or by being awarded a walk (four balls outside the strike zone).\n- A player can also advance around the bases by stealing a base (running to the next base while the pitcher is delivering the ball).\n- A player scores a run when they touch home plate after advancing around all the bases.\n- The pitcher throws the ball from the pitcher's mound to the catcher, who is positioned behind home plate. The batter stands at home plate and tries to hit the ball with a bat.\n- If the batter hits the ball into fair territory, they can run to first base. If they reach first base safely, they can try to advance to second base, third base, and home plate.\n- The fielders try to catch the ball before it hits the ground or to throw the ball to a fielder who can catch it. If a fielder catches the ball before it hits the ground, the batter is out.\n- If the batter hits the ball out of the park (over the fence), they hit a home run and score a run.\n- The game ends after nine innings, or after extra innings if the score is tied. The team with the most runs at the end of the game wins.\n\n**Scoring:**\n- A team scores a run when a player touches home plate after advancing around all the bases.\n- A player can score a run by hitting a home run, by being driven in (batted in) by another player, or by advancing around the bases on an error by the fielding team.\n\n**Outs:**\n- A batter is out if they:\n  -"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEITqYF1QNkP"
   },
   "source": [
    "### Generation: PaLM `text-unicorn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1683726411751,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "ippLc2_eYeQH",
    "outputId": "c09300ed-9e44-40a7-f3d0-f4a95515021c",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:57.422922Z",
     "start_time": "2024-03-27T15:34:52.036349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Baseball is a bat-and-ball game played between two teams of nine players each. The game is played on a field with four bases arranged in a diamond shape. The objective of the game is to score runs by hitting the ball and running around the bases.\n\nThe game begins with one team batting and the other team fielding. The pitcher throws the ball to the batter, who tries to hit it with the bat. If the batter hits the ball, they run to first base. If they reach first base safely, they can continue running to second, third, and home base. If they reach home base, they score"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = text_model_u.predict(question)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:34:57.520247Z",
     "start_time": "2024-03-27T15:34:57.391015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Baseball is a bat-and-ball game played between two teams of nine players each. The game is played on a field with four bases arranged in a diamond shape. The objective of the game is to score runs by hitting the ball and running around the bases.\n\nThe game begins with one team batting and the other team fielding. The pitcher throws the ball to the batter, who tries to hit it with the bat. If the batter hits the ball, they run to first base. If they reach first base safely, they can continue running to second, third, and home base. If they reach home base, they score"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:10.388857Z",
     "start_time": "2024-03-27T15:34:57.400977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing previous run from GCS\n"
     ]
    }
   ],
   "source": [
    "if USE_PRIOR_RUN == False:\n",
    "    PRIOR_PARSE = False\n",
    "    \n",
    "    # do a check for prior run and present message if found letting user know the prior result exists but not being used\n",
    "    if RETRIEVE_FROM == 'GCS' and len(list(bucket.list_blobs(prefix = f'{SERIES}/{EXPERIMENT}/files_pages.json'))) > 0:\n",
    "        print(f'Previous results exists in GCS but forcing the creation of new parsing with USE_PRIOR_RUN = {USE_PRIOR_RUN}')\n",
    "    elif RETRIEVE_FROM == 'BQ' and bq_table_check(f'{BQ_DATASET}.{BQ_TABLE}_files_pages'):\n",
    "        print(f'Previous results exists in BQ but forcing the creation of new parsing with USE_PRIOR_RUN = {USE_PRIOR_RUN}')\n",
    "\n",
    "elif RETRIEVE_FROM == 'GCS' and len(list(bucket.list_blobs(prefix = f'{SERIES}/{EXPERIMENT}/files_pages.json'))) > 0:\n",
    "    print('Importing previous run from GCS')\n",
    "\n",
    "    # load files_pages: the file+page level information including docai responses in `parsing`\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/files_pages.json')\n",
    "    files_pages = [json.loads(line) for line in blob.download_as_text().splitlines()]\n",
    "    \n",
    "    # load files_pages_chunks: the chunks parsed from the files+pages\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/files_pages_chunks.json')\n",
    "    files_pages_chunks = [json.loads(line) for line in blob.download_as_text().splitlines()]   \n",
    "    \n",
    "    # Set Indicator to prevent redoing the parsing later in this notebook\n",
    "    PRIOR_PARSE = True\n",
    "\n",
    "elif RETRIEVE_FROM == 'BQ' and bq_table_check(f'{BQ_DATASET}.{BQ_TABLE}_files_pages'):\n",
    "    print('Importing previous run from BigQuery')\n",
    "\n",
    "    # load files_pages: the file+page level information including docai responses in `parsing`\n",
    "    files_pages = bq.query(f'SELECT * FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}_files_pages` ORDER BY file_index, page_index').to_dataframe().to_dict('records')\n",
    "    # convert json string to dictionary:\n",
    "    for page in files_pages:\n",
    "        page['parsing'] = json.loads(page['parsing'])\n",
    "    \n",
    "    # load files_pages_chunks: the chunks parsed from the files+pages\n",
    "    files_pages_chunks = bq.query(f'SELECT * FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}_files_pages_chunks`').to_dataframe().to_dict('records')\n",
    "    #convert json string to dictionary:\n",
    "    for chunk in files_pages_chunks:\n",
    "        chunk['metadata'] = json.loads(chunk['metadata'])\n",
    "    # sort chunk by file, page, chunk number:\n",
    "    files_pages_chunks = sorted(files_pages_chunks, key = lambda x: (x['metadata']['file_index'], x['metadata']['page_index'], x['metadata']['chunk']))\n",
    "\n",
    "    # Set Indicator to prevent redoing the parsing later in this notebook\n",
    "    PRIOR_PARSE = True\n",
    "        \n",
    "else:\n",
    "    print('No previous run available to import')\n",
    "    PRIOR_PARSE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generation: PaLM `text-bison-32k`"
   ],
   "metadata": {
    "id": "qEITqYF1QNkP"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": " **Objective:**\nThe objective of baseball is to score runs by hitting the ball and advancing around the bases. The team with the most runs at the end of the game wins.\n\n**Gameplay:**\n- A baseball game is played between two teams, each with nine players on the field.\n- The game is divided into nine innings, each consisting of two halves: the top of the inning when the visiting team bats, and the bottom of the inning when the home team bats.\n- Each team takes turns batting and fielding. The batting team tries to hit the ball into fair territory (between the foul lines) and advance around"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = text_model_b32.predict(question)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1683726411751,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "ippLc2_eYeQH",
    "outputId": "c09300ed-9e44-40a7-f3d0-f4a95515021c",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:12.582927Z",
     "start_time": "2024-03-27T15:35:10.356523Z"
    }
   },
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Objective:**\n\nThe goal of baseball is for one team (the offense) to score more runs than the other team (the defense) by hitting a ball and running around bases.\n\n**Setup and Equipment:**\n\n* **Field:** A baseball field is diamond shaped with four bases: home, first, second and third.\n* **Ball and Bat:** The baseball is a small, leather-covered sphere. The bat is made of wood or metal.\n\n**Inning and Game Play:**\n\n* A baseball game consists of nine innings. Each inning is divided into two parts, a top half and a bottom half.\n* The offense (batting team) takes the field first in the top of the inning, trying to score runs by hitting the ball.\n* The defense (fielding team) positions players around the field to try to prevent runs by catching or fielding the ball.\n\n**Hitting:**\n\n* The pitcher from the defense team throws a ball towards the batter from the offense team.\n* If the batter hits a fair ball (within certain lines on the field), they attempt to run to first base.\n\n**Baserunning and Scoring Runs:**\n\n* After a batter hits a fair ball, they run around the bases in order (first, second, third, and finally, home).\n* If the runner reaches home plate before three defensive outs (explained below) are made, the offense scores a point (called a \"run\").\n\n**Outs:**\n\nThere are several ways a defensive team can get an \"out\":\n* **Strikeouts (3):** Three missed swings or balls hit outside of the strike zone.\n* **Groundouts:** A batter hits the ball on the ground and it is fielded and thrown to first base before the batter can reach it.\n* **Flyouts:** A batter hits the ball into the air, and it is caught before it hits the ground.\n* **Tagging out a runner:** Touching a runner with a live ball or a glove containing the ball.\n* **Force outs (rare):** A runner is forced to advance to the next base due to a batter hitting the ball, and the defense completes a play on that base.\n\n**Pitching:**\n\n* The投手(投手隊的球手)向打擊手投球。目標是在好壞球區(好壞球區)投球，並迫使其揮空或擊出好打的球。\n* **好壞球區:**一個想像的盒子狀區域，介於本壘板上方與膝蓋以下、本壘板內緣與外緣間。投手必須將球投進這個區域內。\n* **四壞保送 (4):**投手投出四個壞球，打擊者可以自動上到一壘。\n* **觸身球:**投手投出的球擊中打擊者，打擊者可自動上到一壘。\n\n**其他規則和策略：**\n\n* **盜壘：**一名跑壘員在投手投球時嘗試從一壘推進到二壘或三壘。\n* **野手選擇：**球隊可以選擇不完成對跑者的出局機會，而是選擇將球傳到另一壘，讓其他跑者出界。\n* **犧牲打：**打擊者犧牲自己出界，以推進跑壘員上到另一個壘包。\n* **策略性走壘（策略性跑壘）：**球隊利用跑壘員的速度、技巧和判斷力，以增加得分機會。"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(response.text)"
   ],
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:32.284394Z",
     "start_time": "2024-03-27T15:35:32.115216Z"
    }
   },
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generation: Gemini `gemini-1.0-pro`"
   ],
   "metadata": {
    "id": "qEITqYF1QNkP"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "candidates {\n  content {\n    role: \"model\"\n    parts {\n      text: \"**Objective:**\\n\\n* To score more runs than the opposing team by advancing runners around the bases and touching home plate (the \\\"plate\\\").\\n\\n**Equipment:**\\n\\n* Baseball (a hard leather ball)\\n* Baseball bat\\n* Baseball glove\\n* Baseball field (with four bases)\\n\\n**Gameplay:**\\n\\n**Inning:**\\n* A game consists of nine innings, with each inning divided into two halves: the top and bottom of the inning.\\n\\n**Top of the Inning:**\\n* The visiting team bats while the home team takes the field.\\n* The visiting team\\'s goal is to score runs.\\n\\n**Bottom of the Inning:**\\n* The home team bats while the visiting team takes the field.\\n* The home team\\'s goal is to prevent the visiting team from scoring more runs than them.\\n\\n**Gameplay within an Inning:**\\n\\n**Pitching:**\\n* The pitcher on the fielding team throws the ball over home plate towards the batter on the batting team.\\n\\n**Batting:**\\n* The batter attempts to hit the pitched ball with their bat.\\n\\n**Hitting:**\\n* If the batter successfully hits the ball, they drop their bat and try to run to first base before the fielders can retrieve the ball and tag them out or throw the ball to a fielder who can tag them out.\\n\\n**Running the Bases:**\\n* Once the batter hits the ball, they try to run around the bases in order: first, second, third, and home plate.\\n* Runners on base can advance by stealing bases or by being driven in by a batter who hits the ball into the outfield.\\n\\n**Fielding:**\\n* The fielders try to retrieve the hit ball and either throw the ball to a base to prevent runners from advancing or to tag runners out by touching them with the ball.\\n\\n**Outs:**\\n* Three outs mark the end of an inning for the batting team. Outs can occur in the following ways:\\n    * Strikeout: The batter misses three pitches.\\n    * Caught out: The fielder catches the ball before it hits the ground.\\n    * Force out: A runner is forced to run to the next base because a preceding runner is already on that base.\\n    * Tag out: A fielder tags the runner with the ball before the runner reaches a base.\\n\\n**Runs:**\\n* A run is scored when a runner successfully touches home plate after having advanced around all four bases.\\n\\n**Winning:**\\n* The team with the most runs at the end of the game wins.\"\n    }\n  }\n  finish_reason: STOP\n  safety_ratings {\n    category: HARM_CATEGORY_HATE_SPEECH\n    probability: NEGLIGIBLE\n    probability_score: 0.138345331\n    severity: HARM_SEVERITY_NEGLIGIBLE\n    severity_score: 0.0751764\n  }\n  safety_ratings {\n    category: HARM_CATEGORY_DANGEROUS_CONTENT\n    probability: NEGLIGIBLE\n    probability_score: 0.179530561\n    severity: HARM_SEVERITY_NEGLIGIBLE\n    severity_score: 0.108188957\n  }\n  safety_ratings {\n    category: HARM_CATEGORY_HARASSMENT\n    probability: NEGLIGIBLE\n    probability_score: 0.151027799\n    severity: HARM_SEVERITY_NEGLIGIBLE\n    severity_score: 0.0674237758\n  }\n  safety_ratings {\n    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n    probability: NEGLIGIBLE\n    probability_score: 0.0917069614\n    severity: HARM_SEVERITY_NEGLIGIBLE\n    severity_score: 0.0587765574\n  }\n}\nusage_metadata {\n  prompt_token_count: 5\n  candidates_token_count: 525\n  total_token_count: 530\n}"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = gemini_text.generate_content(question)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1683726411751,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "ippLc2_eYeQH",
    "outputId": "c09300ed-9e44-40a7-f3d0-f4a95515021c",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:39.420727Z",
     "start_time": "2024-03-27T15:35:34.386059Z"
    }
   },
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Objective:**\n",
      "\n",
      "* To score more runs than the opposing team by advancing runners around the bases and touching home plate (the \"plate\").\n",
      "\n",
      "**Equipment:**\n",
      "\n",
      "* Baseball (a hard leather ball)\n",
      "* Baseball bat\n",
      "* Baseball glove\n",
      "* Baseball field (with four bases)\n",
      "\n",
      "**Gameplay:**\n",
      "\n",
      "**Inning:**\n",
      "* A game consists of nine innings, with each inning divided into two halves: the top and bottom of the inning.\n",
      "\n",
      "**Top of the Inning:**\n",
      "* The visiting team bats while the home team takes the field.\n",
      "* The visiting team's goal is to score runs.\n",
      "\n",
      "**Bottom of the Inning:**\n",
      "* The home team bats while the visiting team takes the field.\n",
      "* The home team's goal is to prevent the visiting team from scoring more runs than them.\n",
      "\n",
      "**Gameplay within an Inning:**\n",
      "\n",
      "**Pitching:**\n",
      "* The pitcher on the fielding team throws the ball over home plate towards the batter on the batting team.\n",
      "\n",
      "**Batting:**\n",
      "* The batter attempts to hit the pitched ball with their bat.\n",
      "\n",
      "**Hitting:**\n",
      "* If the batter successfully hits the ball, they drop their bat and try to run to first base before the fielders can retrieve the ball and tag them out or throw the ball to a fielder who can tag them out.\n",
      "\n",
      "**Running the Bases:**\n",
      "* Once the batter hits the ball, they try to run around the bases in order: first, second, third, and home plate.\n",
      "* Runners on base can advance by stealing bases or by being driven in by a batter who hits the ball into the outfield.\n",
      "\n",
      "**Fielding:**\n",
      "* The fielders try to retrieve the hit ball and either throw the ball to a base to prevent runners from advancing or to tag runners out by touching them with the ball.\n",
      "\n",
      "**Outs:**\n",
      "* Three outs mark the end of an inning for the batting team. Outs can occur in the following ways:\n",
      "    * Strikeout: The batter misses three pitches.\n",
      "    * Caught out: The fielder catches the ball before it hits the ground.\n",
      "    * Force out: A runner is forced to run to the next base because a preceding runner is already on that base.\n",
      "    * Tag out: A fielder tags the runner with the ball before the runner reaches a base.\n",
      "\n",
      "**Runs:**\n",
      "* A run is scored when a runner successfully touches home plate after having advanced around all four bases.\n",
      "\n",
      "**Winning:**\n",
      "* The team with the most runs at the end of the game wins.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ],
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:46.842882Z",
     "start_time": "2024-03-27T15:35:46.703938Z"
    }
   },
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Objective:**\n\n* To score more runs than the opposing team by advancing runners around the bases and touching home plate (the \"plate\").\n\n**Equipment:**\n\n* Baseball (a hard leather ball)\n* Baseball bat\n* Baseball glove\n* Baseball field (with four bases)\n\n**Gameplay:**\n\n**Inning:**\n* A game consists of nine innings, with each inning divided into two halves: the top and bottom of the inning.\n\n**Top of the Inning:**\n* The visiting team bats while the home team takes the field.\n* The visiting team's goal is to score runs.\n\n**Bottom of the Inning:**\n* The home team bats while the visiting team takes the field.\n* The home team's goal is to prevent the visiting team from scoring more runs than them.\n\n**Gameplay within an Inning:**\n\n**Pitching:**\n* The pitcher on the fielding team throws the ball over home plate towards the batter on the batting team.\n\n**Batting:**\n* The batter attempts to hit the pitched ball with their bat.\n\n**Hitting:**\n* If the batter successfully hits the ball, they drop their bat and try to run to first base before the fielders can retrieve the ball and tag them out or throw the ball to a fielder who can tag them out.\n\n**Running the Bases:**\n* Once the batter hits the ball, they try to run around the bases in order: first, second, third, and home plate.\n* Runners on base can advance by stealing bases or by being driven in by a batter who hits the ball into the outfield.\n\n**Fielding:**\n* The fielders try to retrieve the hit ball and either throw the ball to a base to prevent runners from advancing or to tag runners out by touching them with the ball.\n\n**Outs:**\n* Three outs mark the end of an inning for the batting team. Outs can occur in the following ways:\n    * Strikeout: The batter misses three pitches.\n    * Caught out: The fielder catches the ball before it hits the ground.\n    * Force out: A runner is forced to run to the next base because a preceding runner is already on that base.\n    * Tag out: A fielder tags the runner with the ball before the runner reaches a base.\n\n**Runs:**\n* A run is scored when a runner successfully touches home plate after having advanced around all four bases.\n\n**Winning:**\n* The team with the most runs at the end of the game wins."
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(response.text)"
   ],
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:48.552838Z",
     "start_time": "2024-03-27T15:35:48.390127Z"
    }
   },
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Retrieve Files From Previous Run on GCS Or BigQuery\n",
    "\n",
    "This uses the input parameter set above: `RETRIEVE_FROM`.  If it is set to `BQ` or `GCS` then it will check the source for an available prior run and retrieve it if it exists.\n"
   ],
   "metadata": {
    "id": "ilogOjNqnkZP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to check for existance of BigQuery Table:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table exists: True, Chunks table exists: True\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "def bq_table_check(bq_client, full_table_id):\n",
    "    \"\"\"\n",
    "    Checks if a specified BigQuery table exists.\n",
    "\n",
    "    Parameters:\n",
    "    - bq_client (bigquery.Client): A BigQuery client object.\n",
    "    - full_table_id (str): The full table ID in the format 'dataset.table_name'.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the table exists, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bq_client.get_table(full_table_id)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Check if specific tables exist within a dataset\n",
    "table_exists = bq_table_check(bq_client, f'{BQ_DATASET}.{BQ_TABLE}_files_pages')\n",
    "chunks_table_exists = bq_table_check(bq_client, f'{BQ_DATASET}.{BQ_TABLE}_files_pages_chunks')\n",
    "\n",
    "print(f\"Table exists: {table_exists}, Chunks table exists: {chunks_table_exists}\")\n"
   ],
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:52.030248Z",
     "start_time": "2024-03-27T15:35:51.375154Z"
    }
   },
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ydo-up2TMwo1"
   },
   "source": [
    "---\n",
    "## Get/Create Document AI Processors\n",
    "\n",
    "Document AI is comprised of multiple processors.  In this case the Form parser is used for its ability to detect and extract tables as well as OCR.  For a more thorough review of Document AI processors, including customized parsers, see the [Working With/Document AI](../Working%20With/Document%20AI/readme.md) section of this repository.  This repository includes example of processing document at larger scales and storing the data for processing and retrieval.\n",
    "\n",
    "Using the [General Form Processor](https://cloud.google.com/document-ai/docs/processors-list#general_processors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:35:53.768277Z",
     "start_time": "2024-03-27T15:35:53.555877Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LOCATION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[72], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m PARSER_TYPE \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFORM_PARSER_PROCESSOR\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m PARSER_VERSION \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpretrained-form-parser-v2.1-2023-06-26\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m docai_client\u001B[38;5;241m.\u001B[39mlist_processors(parent \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprojects/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPROJECT_ID\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/locations/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mLOCATION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mdisplay_name \u001B[38;5;241m==\u001B[39m PARSER_DISPLAY_NAME:\n\u001B[1;32m      7\u001B[0m         parser \u001B[38;5;241m=\u001B[39m p\n",
      "\u001B[0;31mNameError\u001B[0m: name 'LOCATION' is not defined"
     ]
    }
   ],
   "source": [
    "PARSER_DISPLAY_NAME = 'my_general_processor'\n",
    "PARSER_TYPE = 'FORM_PARSER_PROCESSOR'\n",
    "PARSER_VERSION = 'pretrained-form-parser-v2.1-2023-06-26'\n",
    "\n",
    "for p in docai_client.list_processors(parent = f'projects/{PROJECT_ID}/locations/{LOCATION}'):\n",
    "    if p.display_name == PARSER_DISPLAY_NAME:\n",
    "        parser = p\n",
    "try:\n",
    "    print('Retrieved existing parser: ', parser.name)\n",
    "except Exception:\n",
    "    parser = docai_client.create_processor(\n",
    "        parent = f'projects/{PROJECT_ID}/locations/{LOCATION}',\n",
    "        processor = dict(display_name = PARSER_DISPLAY_NAME, type_ = PARSER_TYPE, default_processor_version = PARSER_VERSION)\n",
    "    )\n",
    "    print('Created New Parser: ', parser.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-2fW-ybMwo3"
   },
   "source": [
    "---\n",
    "## Get The Documents\n",
    "\n",
    "Get the source PDF(s) from GCS or a URL and store as a list of pages for each file: `file_pages`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Location of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.097341Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    document_locations = []\n",
    "    for source_document in source_documents:\n",
    "        if source_document.startswith('http'):\n",
    "            document_locations.append('URL')\n",
    "            print(f'Use requests to get online document: {source_document}')\n",
    "        elif source_document.startswith('gs'):\n",
    "            document_locations.append('GCS')\n",
    "            print(f'Use GCS to get document in GCS: {source_document}')\n",
    "        else:\n",
    "            document_locations.append('UNKNOWN')\n",
    "            print(f'The source_document variable points to a document in an unknown location type (not gs:// or http): {source_document}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8PsjZxvTr14"
   },
   "source": [
    "Import the PDF to memory as bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1683726412436,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "MN9SYNb2TsEo",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.104430Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    imported_documents = []\n",
    "    for s, source_document in enumerate(source_documents):\n",
    "        if document_locations[s] == 'URL':\n",
    "            imported_documents.append(requests.get(source_document).content)\n",
    "        elif document_locations[s] == 'GCS':\n",
    "            blob = bucket.blob(source_document.split(f'gs://{GCS_BUCKET}/')[1])\n",
    "            imported_documents.append(blob.download_as_bytes())\n",
    "        elif document_locations[s] == 'UNKNOWN':\n",
    "            imported_documents.append(None)\n",
    "    type(imported_documents[0])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3sEH4iI0Mn_"
   },
   "source": [
    "Convert from bytes to PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683726412438,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "so7ejmvf1U9_",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.112996Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    converted_documents = []\n",
    "    for imported_document in imported_documents:\n",
    "        if imported_document:\n",
    "            converted_documents.append(PyPDF2.PdfReader(io.BytesIO(imported_document)))\n",
    "        else:\n",
    "            converted_documents.append(None)\n",
    "        type(converted_documents[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review number of pages per PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1683726412829,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "iyb-77cz1VCW",
    "outputId": "eec898eb-1e6f-4f51-d9fe-2288002f176d",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.117888Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    for f, file in enumerate(converted_documents):\n",
    "        if file:\n",
    "            print(f\"{source_documents[f]} has {len(file.pages)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split PDF(s) to list of individual pages for each file:\n",
    "\n",
    "List of dictionaries with keys: file_index, page_index, raw_file_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.123821Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    # list of tuples (file index, page number, page content)\n",
    "    files_pages = []\n",
    "    for c, converted_document in enumerate(converted_documents):\n",
    "        if converted_document:\n",
    "            for page_num, page in enumerate(converted_document.pages, 1):\n",
    "                writer = PyPDF2.PdfWriter()\n",
    "                writer.add_page(page)\n",
    "                with io.BytesIO() as bytes_stream:\n",
    "                    files_pages.append(\n",
    "                        dict(file_index = c, page_index = page_num, raw_file_page = writer.write(bytes_stream)[1].getbuffer().tobytes())\n",
    "                    )\n",
    "len(files_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph2FjuLWnRgD"
   },
   "source": [
    "---\n",
    "## Parse Documents\n",
    "\n",
    "Results of:\n",
    "- [google.cloud.documentai.DocumentProcessorServiceClient().process_document()](https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.services.document_processor_service.DocumentProcessorServiceClient#google_cloud_documentai_v1_services_document_processor_service_DocumentProcessorServiceClient_process_document)\n",
    "  - are in the format of\n",
    "    - [google.cloud.documentai_v1.types.ProcessResponse()](https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.types.ProcessResponse)\n",
    "      - which contains `.document` in the format of:\n",
    "        - [google.cloud.documentai_v1.types.Document](https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.types.Document)\n",
    "\n",
    "Converting the Document to:\n",
    "- JSON with .to_json()\n",
    "- dictionary with .to_dict()\n",
    "\n",
    "**Document AI Notes:**\n",
    "- In this application we are using online processing.  This has a limit of 15 pages per document.  Switch to batch increases this to 100 pages for the Form Parser (General).\n",
    "- Online processing has a default qouta of 120 requests per minute per project. The code below implements waiting time to avoid this limit.\n",
    "- [Reference](https://cloud.google.com/document-ai/quotas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.132074Z"
    }
   },
   "outputs": [],
   "source": [
    "async def docai_runner(files_pages, limit_concur_requests = 120):\n",
    "    limit = asyncio.Semaphore(limit_concur_requests)\n",
    "    results = [None] * len(files_pages)\n",
    "    \n",
    "    # make requests - async\n",
    "    async def make_request(p):\n",
    "        \n",
    "        async with limit:\n",
    "            if limit.locked():\n",
    "                await asyncio.sleep(0.01)\n",
    "                \n",
    "            ########### manual Error Handling ############################################\n",
    "            fail_count = 0\n",
    "            while fail_count <= 20:\n",
    "                try:\n",
    "                    result = await docai_async_client.process_document(\n",
    "                        request = dict(\n",
    "                            raw_document = documentai.RawDocument(\n",
    "                                content = files_pages[p]['raw_file_page'],\n",
    "                                mime_type = 'application/pdf'\n",
    "                            ),\n",
    "                            name = parser.name\n",
    "                        )\n",
    "                    )\n",
    "                    if fail_count > 0:\n",
    "                        print(f'Item {p} succeeded after fail count = {fail_count}')\n",
    "                    break\n",
    "                except:\n",
    "                    fail_count += 1\n",
    "                    #print(f'Item {p} failed: current fail count = {fail_count}')\n",
    "                    await asyncio.sleep(2^(min(fail_count, 6) - 1))\n",
    "            ##############################################################################\n",
    "            \n",
    "        results[p] = documentai.Document.to_dict(result.document)\n",
    "    \n",
    "    # manage tasks\n",
    "    tasks = [asyncio.create_task(make_request(p)) for p in range(len(files_pages))]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # add parsing to input list of dictionaries for all the pages\n",
    "    for c, content in enumerate(files_pages):\n",
    "        content['parsing'] = results[c]\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.137270Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    print('No Prior Results, Parsing with Document AI')\n",
    "    await docai_runner(files_pages)\n",
    "    # remove the raw file page\n",
    "    for page in files_pages: del page['raw_file_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.145594Z"
    }
   },
   "outputs": [],
   "source": [
    "len(files_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.151428Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.160120Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages[0]['parsing'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pages dictionary, add the path, file, page, and embedding of the full full pages OCR results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.169190Z"
    }
   },
   "outputs": [],
   "source": [
    "async def embedding_pages(files_pages, limit_concur_requests = 500):\n",
    "    limit = asyncio.Semaphore(limit_concur_requests)\n",
    "    results = [None] * len(files_pages)\n",
    "    \n",
    "    # make requests - async\n",
    "    async def make_request(p):\n",
    "        \n",
    "        async with limit:\n",
    "            if limit.locked():\n",
    "                await asyncio.sleep(0.01)\n",
    "                \n",
    "            ########### manual Error Handling ############################################\n",
    "            fail_count = 0\n",
    "            while fail_count <= 20:\n",
    "                try:\n",
    "                    if files_pages[p]['parsing']['text']:\n",
    "                        result = await textembed_model.get_embeddings_async([files_pages[p]['parsing']['text']])\n",
    "                    else:\n",
    "                        obj = lambda: None\n",
    "                        obj.values = [None]\n",
    "                        result = [obj]\n",
    "                    \n",
    "                    if fail_count > 0:\n",
    "                        print(f'Item {p} succeeded after fail count = {fail_count}')\n",
    "                    break\n",
    "                except:\n",
    "                    fail_count += 1\n",
    "                    #print(f'Item {p} failed: current fail count = {fail_count}')\n",
    "                    await asyncio.sleep(2^(min(fail_count, 6) - 1))\n",
    "            ##############################################################################\n",
    "            \n",
    "        results[p] = result[0].values\n",
    "    \n",
    "    # manage tasks\n",
    "    tasks = [asyncio.create_task(make_request(p)) for p in range(len(files_pages))]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for c, content in enumerate(files_pages):\n",
    "        content['parsing']['embedding'] = results[c]\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.175475Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    print('No Prior Results, Using Document AI Parsing')\n",
    "    await embedding_pages(files_pages)\n",
    "    for c, content in enumerate(files_pages):\n",
    "        document_image = PIL.Image.open(\n",
    "            io.BytesIO(\n",
    "                base64.decodebytes(content['parsing']['pages'][0]['image']['content'].encode('utf-8'))\n",
    "            )\n",
    "        )\n",
    "        content['parsing']['path'] = source_documents[content['file_index']][:(-1*len(source_documents[content['file_index']].split('/')[-1]))]\n",
    "        content['parsing']['file'] = source_documents[content['file_index']].split('/')[-1]\n",
    "        content['parsing']['page'] = content['page_index']\n",
    "        content['parsing']['vme_id'] = f\"{content['file_index']}_{content['page_index']}\"\n",
    "        content['parsing']['dimensions'] = list(document_image.size)\n",
    "        if not content['parsing']['text']:\n",
    "            content['parsing']['embedding'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.180210Z"
    }
   },
   "outputs": [],
   "source": [
    "len(files_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.185722Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.190799Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages[0]['parsing'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Chunks From Documents\n",
    "\n",
    "Elements to capture here are paragraphs and tables.  If a paragraph overlaps a table then include it within the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.195247Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Document Preparation')\n",
    "else:\n",
    "    files_pages_chunks = []\n",
    "    \n",
    "    for content in files_pages:\n",
    "        page = content['parsing']\n",
    "        chunk_id = 0\n",
    "        \n",
    "        # parse tables from page:\n",
    "        tables = []\n",
    "        for t, table in enumerate(page['pages'][0]['tables']):\n",
    "\n",
    "            table_txt = ''\n",
    "            if 'text_anchor' in table['layout'].keys():\n",
    "                for s, segment in enumerate(table['layout']['text_anchor']['text_segments']):\n",
    "                    if t == 0 and s == 0: start = 0\n",
    "                    else: start = int(segment['start_index'])\n",
    "                    end = int(segment['end_index'])\n",
    "                    table_txt += page['text'][start:end+t]\n",
    "\n",
    "            vertices = []\n",
    "            normalized_vertices = []\n",
    "            for vertex in table['layout']['bounding_poly']['normalized_vertices']:\n",
    "                normalized_vertices.append(dict(x = vertex['x'], y = vertex['y']))\n",
    "                vertices.append(dict(x = vertex['x'] * page['dimensions'][0], y = vertex['y'] * page['dimensions'][1]))\n",
    "            tables.append(shapely.geometry.Polygon([(v['x'], v['y']) for v in vertices]))\n",
    "\n",
    "            if table_txt != '':\n",
    "                files_pages_chunks.append(\n",
    "                    dict(\n",
    "                        text = table_txt,\n",
    "                        metadata = dict(\n",
    "                            file_index = content['file_index'],\n",
    "                            page_index = content['page_index'],\n",
    "                            table = t + 1,\n",
    "                            chunk = chunk_id + 1,\n",
    "                            vme_id = page['vme_id'] + '_' + str(chunk_id),\n",
    "                            vertices = vertices,\n",
    "                            normalized_vertices = normalized_vertices\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                chunk_id += 1       \n",
    "        \n",
    "        # parse paragraphs from page - not in tables or overlapping tables\n",
    "        for g, paragraph in enumerate(page['pages'][0]['paragraphs']):\n",
    "\n",
    "            # get the paragraph text\n",
    "            paragraph_txt = ''\n",
    "            for s, segment in enumerate(paragraph['layout']['text_anchor']['text_segments']):\n",
    "                if g == 0 and s == 0: start = 0\n",
    "                else: start = int(segment['start_index'])\n",
    "                end = int(segment['end_index'])\n",
    "                paragraph_txt += page['text'][start:end+1]\n",
    "\n",
    "            # if paragraph not empty, get/calc the vertices\n",
    "            if paragraph_txt != '':\n",
    "                use_paragraph = True\n",
    "                vertices = []\n",
    "                normalized_vertices = []\n",
    "                for vertex in paragraph['layout']['bounding_poly']['normalized_vertices']:\n",
    "                    normalized_vertices.append(dict(x = vertex['x'], y = vertex['y']))\n",
    "                    vertices.append(dict(x = vertex['x'] * page['dimensions'][0], y = vertex['y'] * page['dimensions'][1]))\n",
    "            else:\n",
    "                use_paragraph = False\n",
    "\n",
    "            # only use paragraphs that are not within/overlapping table boundaries\n",
    "            if use_paragraph:\n",
    "                for t_shape in tables:\n",
    "                    p_shape = shapely.geometry.Polygon([(v['x'], v['y']) for v in vertices])\n",
    "                    if p_shape.intersects(t_shape):\n",
    "                        use_paragraph = False\n",
    "\n",
    "            # save the paragraph as an element\n",
    "            if use_paragraph:\n",
    "                files_pages_chunks.append(\n",
    "                    dict(\n",
    "                        text = paragraph_txt,\n",
    "                        metadata = dict(\n",
    "                            file_index = content['file_index'],\n",
    "                            page_index = content['page_index'],\n",
    "                            paragraph = g + 1,\n",
    "                            chunk = chunk_id +1,\n",
    "                            vme_id = page['vme_id'] + '_' + str(chunk_id),\n",
    "                            vertices = vertices,\n",
    "                            normalized_vertices = normalized_vertices\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                chunk_id += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.199850Z"
    }
   },
   "outputs": [],
   "source": [
    "len(files_pages_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.204490Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.208750Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0]['metadata'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PUA8dwSKtzl"
   },
   "source": [
    "---\n",
    "## Get Embeddings\n",
    "\n",
    "\n",
    "The `textembedding-gecko` model has quota of 1500 request per minute:\n",
    "- [Quotas by region and model](https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#quotas_by_region_and_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.213048Z"
    }
   },
   "outputs": [],
   "source": [
    "async def embedding_runner(files_pages_chunks, limit_concur_requests = 500):\n",
    "    limit = asyncio.Semaphore(limit_concur_requests)\n",
    "    results = [None] * len(files_pages_chunks)\n",
    "    \n",
    "    # make requests - async\n",
    "    async def make_request(p):\n",
    "        \n",
    "        async with limit:\n",
    "            if limit.locked():\n",
    "                await asyncio.sleep(0.01)\n",
    "                \n",
    "            ########### manual Error Handling ############################################\n",
    "            fail_count = 0\n",
    "            while fail_count <= 20:\n",
    "                try:\n",
    "                    result = await textembed_model.get_embeddings_async([files_pages_chunks[p]['text']])\n",
    "                    if fail_count > 0:\n",
    "                        print(f'Item {p} succeeded after fail count = {fail_count}')\n",
    "                    break\n",
    "                except:\n",
    "                    fail_count += 1\n",
    "                    #print(f'Item {p} failed: current fail count = {fail_count}')\n",
    "                    await asyncio.sleep(2^(min(fail_count, 6) - 1))\n",
    "            ##############################################################################\n",
    "            \n",
    "        results[p] = result[0].values\n",
    "    \n",
    "    # manage tasks\n",
    "    tasks = [asyncio.create_task(make_request(p)) for p in range(len(files_pages_chunks))]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # add embeddings to input list of dictionaries for all the chunks\n",
    "    for c, content in enumerate(files_pages_chunks):\n",
    "        content['embedding'] = results[c]\n",
    "    \n",
    "    await asyncio.sleep(60)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.217780Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Embeddings created on previous run.')\n",
    "else:\n",
    "    await embedding_runner(files_pages_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.222292Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.226711Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0]['metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.231131Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0]['embedding'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udUMCH-ponRL"
   },
   "source": [
    "---\n",
    "## Save Files For Future Runs: GCS, BigQuery\n",
    "\n",
    "Use the values of the input parameter `SAVE_IN` to optionally write both `results` and `documents` to `BQ`, `GCS` or `ALL` (both).\n",
    "\n",
    "It can take awhile to run the parsing job above so save results for future runs of this notebook.  Also, this prevents recurring cost of running the Document AI parsing of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.235561Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.239959Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.244050Z"
    }
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('This run loaded results from a prior run.  Not overwriting.')\n",
    "else:\n",
    "    if SAVE_IN in ['GCS', 'ALL']:\n",
    "        print('Writing contents of results and documents to GCS for future use.')\n",
    "\n",
    "        # save files_pages: json lines                    \n",
    "        blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/files_pages.json')\n",
    "        blob.upload_from_string('\\n'.join([json.dumps(page) for page in files_pages]), content_type = 'application/json')\n",
    "\n",
    "        # save files_pages_elements: json lines\n",
    "        blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/files_pages_chunks.json')\n",
    "        blob.upload_from_string('\\n'.join([json.dumps(chunk) for chunk in files_pages_chunks]), content_type = 'application/json')\n",
    "\n",
    "    if SAVE_IN in ['BQ', 'ALL']:\n",
    "        print('Writing contents of results and documents to BigQuery for future use.')\n",
    "\n",
    "        # create/link to dataset\n",
    "        ds = bigquery.DatasetReference(BQ_PROJECT, BQ_DATASET)\n",
    "        ds.location = BQ_REGION\n",
    "        ds.labels = {'series': f'{SERIES}', 'experiment': f'{EXPERIMENT}'}\n",
    "        ds = bq.create_dataset(dataset = ds, exists_ok = True)  \n",
    "\n",
    "        # save files_pages\n",
    "        load_job = bq.load_table_from_json(\n",
    "            json_rows = files_pages,\n",
    "            destination = ds.table(BQ_TABLE + '_files_pages'),\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "                write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE, #.WRITE_APPEND, #.WRITE_TRUNCATE, #.WRITE_EMPTY\n",
    "                create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED, #.CREATE_NEVER\n",
    "                #schema_update_options = [bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
    "                #autodetect = True\n",
    "                schema = [\n",
    "                    bigquery.SchemaField(\"file_index\", \"INT64\"),\n",
    "                    bigquery.SchemaField(\"page_index\", \"INT64\"),\n",
    "                    bigquery.SchemaField(\"parsing\", \"JSON\")\n",
    "                ]\n",
    "            ) \n",
    "        )\n",
    "        load_job.result()\n",
    "        \n",
    "        # save files_pages_chunks\n",
    "        load_job = bq.load_table_from_json(\n",
    "            json_rows = files_pages_chunks,\n",
    "            destination = ds.table(BQ_TABLE + '_files_pages_chunks'),\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "                write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE, #.WRITE_APPEND, #.WRITE_TRUNCATE, #.WRITE_EMPTY\n",
    "                create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED, #.CREATE_NEVER\n",
    "                #schema_update_options = [bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
    "                #autodetect = True\n",
    "                schema = [\n",
    "                    bigquery.SchemaField(\"text\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"metadata\", \"JSON\"),\n",
    "                    bigquery.SchemaField(\"embedding\", \"FLOAT\", \"REPEATED\")\n",
    "                ]\n",
    "            ) \n",
    "        )\n",
    "        load_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Embeddings Search: AKA Vector Search\n",
    "\n",
    "There are many ways to do vector search. In production there are considerations, including:\n",
    "- How many indexes?\n",
    "- What is the size of indexes?\n",
    "- What is the lifespan and frequency of indexes?\n",
    "- How frequently will indexes get updates (append, update, delete)?\n",
    "- How quickly do updates need to surface in searches?\n",
    "- What is the next step after a search?  Does the coorespondinng text need to be retrieved separately?\n",
    "\n",
    "This notebook is designed to show the workflow across a few documents and uses brute for search of all embeddings locally using the common Python package `numpy`.  But what about scaling in production?\n",
    "\n",
    "Google Cloud offers solutions for any workflow!\n",
    "- Local to the application:\n",
    "    - [ScAAN](https://github.com/google-research/google-research/tree/master/scann)\n",
    "    - [Faiss](https://github.com/facebookresearch/faiss)\n",
    "- With transactional data:\n",
    "    - using [pgvector](https://github.com/pgvector/pgvector) with:\n",
    "        - [Cloud SQL for PostgreSQL](https://cloud.google.com/sql/docs/postgres)\n",
    "        - [AlloyDB for PostgreSQL](https://cloud.google.com/alloydb/docs)\n",
    "        - blogs:\n",
    "            - [Building AI-powered apps on Google Cloud databases using pgvector, LLMs and LangChain](https://cloud.google.com/blog/products/databases/using-pgvector-llms-and-langchain-with-google-cloud-databases)\n",
    "    - [Spanner](https://cloud.google.com/spanner/docs)\n",
    "        - [Vector Search in Spanner](https://cloud.google.com/spanner/docs/find-k-nearest-neighbors)\n",
    "        - [langchain with Spanner](https://github.com/googleapis/langchain-google-spanner-python)\n",
    "- In the data warehouse:\n",
    "    - [BigQuery Vector Indexes](https://cloud.google.com/bigquery/docs/vector-search-intro)\n",
    "- Fit-for-purpose: Fast, Scalable, and Flexible:\n",
    "    - [Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview) with built-in [Search using embeddings](https://cloud.google.com/vertex-ai/docs/featurestore/latest/embeddings-search) \n",
    "    - [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embedding for Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.248228Z"
    }
   },
   "outputs": [],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.253226Z"
    }
   },
   "outputs": [],
   "source": [
    "query_embed = np.array(textembed_model.get_embeddings([question])[0].values)\n",
    "query_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Local Embedding DB - With Numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.259082Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_db = np.array([chunk['embedding'] for chunk in files_pages_chunks])\n",
    "embed_db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Matches - With Numpy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product\n",
    "\n",
    "- measures alignment between two vectors\n",
    "- large positive implies similar directions\n",
    "- large negative implies opposite directions\n",
    "- near zero implies orthoganal\n",
    "- larger is more similar\n",
    "- best for:\n",
    "    - matching and retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.263693Z"
    }
   },
   "outputs": [],
   "source": [
    "similarity = np.dot(query_embed, embed_db.T)\n",
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.267894Z"
    }
   },
   "outputs": [],
   "source": [
    "# larger is more similar\n",
    "matches = np.argsort(similarity)[::-1][:5].tolist()\n",
    "matches = [(match, similarity[match]) for match in matches]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.272134Z"
    }
   },
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    print(files_pages_chunks[match[0]]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "- measures angle between two vectors\n",
    "- represents the cosine of the angles resulting in values -1 to 1\n",
    "- larger is more similar\n",
    "- best for:\n",
    "    - grouping for topics: different magnitudes can still group together\n",
    "    - collaborative filtering for recommendation systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.276521Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity = similarity / (np.linalg.norm(query_embed) * np.linalg.norm(embed_db, axis = 1).T)\n",
    "cosine_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.280766Z"
    }
   },
   "outputs": [],
   "source": [
    "# larger is more similar\n",
    "matches = np.argsort(cosine_similarity)[::-1][:5].tolist()\n",
    "matches = [(match, cosine_similarity[match]) for match in matches]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.283715Z"
    }
   },
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    print(files_pages_chunks[match[0]]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance\n",
    "\n",
    "- straight line distance between two vector points\n",
    "- smaller is more similar\n",
    "- note that smaller magnitude vectors with large angles may be deemed more similar than larger magnitude vectors with small angles\n",
    "- best for:\n",
    "    - clustering points in vector space\n",
    "    - anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.286698Z"
    }
   },
   "outputs": [],
   "source": [
    "euclidean_similarity = np.linalg.norm(embed_db - query_embed, axis = 1)\n",
    "euclidean_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.289785Z"
    }
   },
   "outputs": [],
   "source": [
    "# smaller is more similar\n",
    "matches = np.argsort(euclidean_similarity)[:5].tolist()\n",
    "matches = [(match, euclidean_similarity[match]) for match in matches]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.293325Z"
    }
   },
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    print(files_pages_chunks[match[0]]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Function: Pages\n",
    "\n",
    "Using dot product, create an embedding database and accompanying search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.296321Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_embed_db = np.array([page['parsing']['embedding'] if page['parsing']['embedding'] else [0]*768 for page in files_pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.299196Z"
    }
   },
   "outputs": [],
   "source": [
    "def page_match(query):\n",
    "    query_embed = np.array(textembed_model.get_embeddings([query])[0].values)\n",
    "    similarity = np.dot(query_embed, pages_embed_db.T)\n",
    "    matches = np.argsort(similarity)[::-1].tolist()\n",
    "    # algorithm to dynamically pick k\n",
    "    k = 1 + 3*int(10*(1-similarity[matches[0]]))\n",
    "    matches = [(match, similarity[match]) for match in matches[0:k]]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.302250Z"
    }
   },
   "outputs": [],
   "source": [
    "page_match(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Function: Chunks\n",
    "\n",
    "Using dot product, create an embedding database and accompanying search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.305149Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks_embed_db = np.array([chunk['embedding'] for chunk in files_pages_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.308069Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_match(query):\n",
    "    query_embed = np.array(textembed_model.get_embeddings([query])[0].values)\n",
    "    similarity = np.dot(query_embed, chunks_embed_db.T)\n",
    "    matches = np.argsort(similarity)[::-1].tolist()\n",
    "    # algorithm to dynamically pick k\n",
    "    k = 1 + 3*int(10*(1-similarity[matches[0]]))\n",
    "    matches = [(match, similarity[match]) for match in matches[0:k]]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.312688Z"
    }
   },
   "outputs": [],
   "source": [
    "chunk_match(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualize Relationships Between Chunks\n",
    "\n",
    "The embeddings are incredibly high dimensional - 768!  To reduce this to a lower dimension, like 2 for plotting, the method [`t-SNE`](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) will be used.  This method models each embedding with a lower 2-dimensional point in a way that similar embeddings are modeled by nearby poiints and dissimilar embeddings by farther points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualizing Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.315635Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=0, n_iter=1000, init = 'pca', learning_rate = 'auto')\n",
    "tsne_results = tsne.fit_transform(chunks_embed_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.319289Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(data = tsne_results, columns = [\"TSNE1\", \"TSNE2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.322699Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(data = df_tsne, x = 'TSNE1', y = 'TSNE2')\n",
    "plt.title(\"Scatter plot of chunks using t-SNE\")\n",
    "plt.xlabel(\"TSNE1\")\n",
    "plt.ylabel(\"TSNE2\")\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Chunks - Color Codeded By Similarity To Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.326933Z"
    }
   },
   "outputs": [],
   "source": [
    "query_embed = np.array(textembed_model.get_embeddings([question])[0].values)\n",
    "similarity = np.dot(query_embed, chunks_embed_db.T)\n",
    "df_tsne['similarity'] = similarity.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.330327Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(data = df_tsne, x = 'TSNE1', y = 'TSNE2', hue = 'similarity', palette = sns.color_palette('coolwarm', as_cmap=True))\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.title(\"Scatter plot of chunks using t-SNE\\nWith Similarity To Prompt Color Coding\")\n",
    "plt.xlabel(\"TSNE1\")\n",
    "plt.ylabel(\"TSNE2\")\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Chunks - Call Out Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.333082Z"
    }
   },
   "outputs": [],
   "source": [
    "matches = chunk_match(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.336115Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tsne.iloc[[match[0] for match in matches]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.339417Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(data = df_tsne, x = 'TSNE1', y = 'TSNE2', hue = 'similarity', palette = sns.color_palette('coolwarm', as_cmap=True))\n",
    "sns.scatterplot(data = df_tsne.iloc[[match[0] for match in matches]], x=\"TSNE1\", y=\"TSNE2\", color = 'black', marker = 's', s = 100)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.title(\"Scatter plot of chunks using t-SNE\\nWith Similarity To Prompt Color Coding\\nMatches Marked With Black Squares\")\n",
    "plt.xlabel(\"TSNE1\")\n",
    "plt.ylabel(\"TSNE2\")\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Structure of Objects: `files_pages`, `files_pages_elements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.343799Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.346879Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages[0]['parsing'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.350136Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.353789Z"
    }
   },
   "outputs": [],
   "source": [
    "files_pages_chunks[0]['metadata'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM__fmk2p6tp"
   },
   "source": [
    "---\n",
    "## Q&A With DocumentBot\n",
    "\n",
    "Make a function that receives the users questions and:\n",
    "- finds and retrieves relative sections of the rules\n",
    "- prepares a prompt for Vertex AI Generative AI that includes the question and the context = sections of document\n",
    "- Retrieves the response (answer) from Vertex AI Generative AI\n",
    "- Retrieves the closest match section of the rules to the response/answer.\n",
    "- Prepares and presents all the information back to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions For Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Functions\n",
    "\n",
    "These retrieve context.\n",
    "\n",
    "**NOTE**: `get_retrieval` calls `expand_retrieval` which call `get_retrieval`.  This can lead to infinate recursion but is prevent by `expand_retrieval` calling `get_retrieval` with `DISTANCE` left at default to prevent a further call to `expand_retrieval`.  Max recursion is 1 in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.358275Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_chunks(query, k = -1, simk = -1):\n",
    "    # k set the number of matches to retrieve, regarless of similarity. k = -1 will trigger calculating k dynamically.\n",
    "    # simk sets a threshold for similarity: <=0 uses k, (0,1] will get all matches with similarity in range [1-simk, 1] \n",
    "    \n",
    "    query_embed = np.array(textembed_model.get_embeddings([query])[0].values)\n",
    "    similarity = np.dot(query_embed, chunks_embed_db.T) # for dot product, higher is better match, since normalized embeddings 1 is best, 0 is worst\n",
    "    matches = np.argsort(similarity)[::-1].tolist()\n",
    "    \n",
    "    if k <= 0:\n",
    "        # algorithm to dynamically pick k\n",
    "        k = 1 + 3*int(10*(1-similarity[matches[0]]))\n",
    "    if simk <= 0:\n",
    "        matches = [(match, similarity[match]) for match in matches[0:k]]\n",
    "    elif simk > 0 and simk <= 1:\n",
    "        indicies = np.where(similarity >= 1-simk)[0]\n",
    "        matches = [(i, similarity[i]) for i in indicies]\n",
    "     \n",
    "    return matches\n",
    "\n",
    "def expand_retrieval(contexts, DISTANCE):\n",
    "    \n",
    "    additional_contexts = []\n",
    "    if DISTANCE > 0:\n",
    "        \n",
    "        # for each page look for surrounding chunks, collect chunks\n",
    "        chunk_indexes = []\n",
    "        for context in contexts:\n",
    "            # get matches for the page from contexts\n",
    "            matches = get_retrieval(context[2], simk = DISTANCE, file_page = (context[3]['file_index'], context[3]['page_index']))\n",
    "            for match in matches:\n",
    "                if match[0] not in chunk_indexes and match[0] not in [c[0] for c in contexts]:\n",
    "                    chunk_indexes += [match[0]]\n",
    "                    additional_contexts.append(match)\n",
    "\n",
    "    return additional_contexts\n",
    "\n",
    "def get_retrieval(question, k = -1, simk = -1, DISTANCE = 0, file_page = None):\n",
    "\n",
    "    if file_page: # this is from a call to this function by expand_retrieval\n",
    "        matches = [match + (files_pages_chunks[match[0]]['text'], files_pages_chunks[match[0]]['metadata'], True) for match in get_chunks(question, k = k, simk = simk) if file_page == (files_pages_chunks[match[0]]['metadata']['file_index'], files_pages_chunks[match[0]]['metadata']['page_index'])]\n",
    "    else: # this is from a call to this function by the main function: document_bot\n",
    "        matches = [match + (files_pages_chunks[match[0]]['text'], files_pages_chunks[match[0]]['metadata'], False) for match in get_chunks(question, k = k, simk = simk)]\n",
    "    \n",
    "    if DISTANCE > 0:\n",
    "        matches = matches + expand_retrieval(matches, DISTANCE)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation Functions\n",
    "\n",
    "This function prepares the prompt by also adding retrieved context = augmenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.362193Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_augmented(question, contexts, ground):\n",
    "    prompt = ''\n",
    "    \n",
    "    if ground:\n",
    "        prompt += \"Give a detailed answer to the question using only the information from the numbered contexts provided below.\"\n",
    "        prompt += \"\\n\\nContexts:\\n\"\n",
    "        prompt += \"\\n\".join([f'  * Context {c+1}: \"{context[2]}\"' for c, context in enumerate(contexts)])\n",
    "        prompt += \"\\n\\nQuestion: \" + question\n",
    "    else:\n",
    "        prompt += \"Question: \" + question\n",
    "        \n",
    "    # add the trigger to the prompt.  In this case, also include the zero shot chain of thought prompt \"think step by step\".\n",
    "    prompt += \"\\n\\nAnswer the question and give and explanation. Think step by step.\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation Functions\n",
    "\n",
    "These functions interact with LLMs to create responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.366822Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_gemini(prompt, genconfigs, model):\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config = vertexai.generative_models.GenerationConfig(\n",
    "            **genconfigs\n",
    "            \n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        text = response.text\n",
    "    except Exception:\n",
    "        text = None\n",
    "\n",
    "    counter = 0\n",
    "    while not text:\n",
    "        genconfigs['temperature'] = .5 - counter*.1\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config = vertexai.generative_models.GenerationConfig(\n",
    "                **genconfigs\n",
    "\n",
    "            )\n",
    "        )\n",
    "        try:\n",
    "            text = response.text\n",
    "        except Exception:\n",
    "            text = None\n",
    "            counter += 1\n",
    "\n",
    "        if counter == 6:\n",
    "            text = 'Please check the prompt, it appears the response is getting blocked.'\n",
    "    \n",
    "    return text\n",
    "\n",
    "def generate_palm(prompt, genconfigs, model):\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        **genconfigs\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "def get_generation(prompt, max_output_tokens, model):\n",
    "    models = dict(GEMINI = gemini_text, PALM_BISON = text_model_b, PALM_BISION32k = text_model_b32, PALM_UNICORN = text_model_u)\n",
    "    \n",
    "    genconfigs = dict(max_output_tokens = max_output_tokens)\n",
    "    \n",
    "    if model == 'GEMINI':\n",
    "        response = generate_gemini(prompt, genconfigs, models[model])\n",
    "    else:\n",
    "        response = generate_palm(prompt, genconfigs, models[model])\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presentation Functions\n",
    "\n",
    "These prepare the response for presentation - and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.369727Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a font to use for annotating the page images:\n",
    "# get font for annotations: get fonts from fonts.google.com\n",
    "font_source_url = \"https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap\"\n",
    "font_source = requests.get(font_source_url).content.decode(\"utf-8\")\n",
    "start_url = font_source.find('url(')+4\n",
    "end_url = font_source.find(')', start_url)\n",
    "font_url = font_source[start_url:end_url]\n",
    "font = PIL.ImageFont.truetype(io.BytesIO(requests.get(font_url).content), 35)\n",
    "\n",
    "def get_presentation(question, contexts, DISTANCE, response, display_contexts, display_annotations):\n",
    "    # repeat the question\n",
    "    IPython.display.display(IPython.display.Markdown(f'**The Question:**\\n\\n{question}\\n\\n'))\n",
    "    \n",
    "    # show the answer\n",
    "    IPython.display.display(IPython.display.Markdown(f'**The Response:**\\n\\n{response}\\n\\n'))\n",
    "    \n",
    "    if display_contexts:\n",
    "    # display the contexts information: page, similarity, hyperlink\n",
    "        context_pres = '**Sources:**\\n\\n'\n",
    "        pages = []\n",
    "        context_types = [c[4] for c in contexts]\n",
    "        if DISTANCE > 0:\n",
    "            context_pres += f'Note: The {len(contexts) - sum(context_types)} contexts were expanded to gather {sum(context_types)} additional chunks on pages with matches using a similarity distance of {DISTANCE}.\\n'\n",
    "        for context in contexts:\n",
    "            page = next([d['parsing']['path'], d['parsing']['file'], d['parsing']['page'], d['file_index'], d['page_index']] for d in files_pages if d['file_index'] == context[3]['file_index'] and d['page_index'] == context[3]['page_index'])\n",
    "            pages.append(page)\n",
    "            if not context[4]:\n",
    "                context_pres += f'1. {page[0]}{page[1]}#page={page[2]}\\n\\t* page: {page[2]}, similarity to question is {context[1]:.3f}\\n'\n",
    "            # the following is commented out, if uncommented it would also add the expanded contexts to printed list (this can be very long for DISTANCE = 1 which is the full page)\n",
    "            #else:\n",
    "            #    context_pres += f'1. {page[0]}{page[1]}#page={page[2]}\\n\\t* page: {page[2]}, similarity to primary context is {context[1]:.3f}\\n'\n",
    "        IPython.display.display(IPython.display.Markdown(context_pres))\n",
    "        \n",
    "    if display_annotations:\n",
    "    # display each page with annotations\n",
    "        IPython.display.display(IPython.display.Markdown('**Annotated Document Pages**\\n\\n'))\n",
    "        # list of unique pages across contexts: sorted list of tuple(file_index, page_index)\n",
    "        pages = sorted(list(set([(page[3], page[4]) for page in pages])), key = lambda x: (x[0], x[1]))\n",
    "        # list of PIL images for each unique page\n",
    "        images = []\n",
    "        for page in pages:\n",
    "            image = next(d['parsing']['pages'][0]['image']['content'] for d in files_pages if d['file_index'] == page[0] and d['page_index'] == page[1])\n",
    "            images.append(\n",
    "                PIL.Image.open(\n",
    "                    io.BytesIO(\n",
    "                        base64.decodebytes(\n",
    "                            image.encode('utf-8')\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        # annotate the contexts on the pages:\n",
    "        for c, context in enumerate(contexts):\n",
    "            image = images[pages.index((context[3]['file_index'], context[3]['page_index']))]\n",
    "            vertices = context[3]['vertices']\n",
    "            draw = PIL.ImageDraw.Draw(image)\n",
    "            if not context[4]: \n",
    "                color = 'green'\n",
    "                prefix = 'Source'\n",
    "            else:\n",
    "                color = 'blue'\n",
    "                prefix = 'Expanded Source'\n",
    "            draw.polygon([\n",
    "                vertices[0]['x'], vertices[0]['y'],\n",
    "                vertices[1]['x'], vertices[1]['y'],\n",
    "                vertices[2]['x'], vertices[2]['y'],\n",
    "                vertices[3]['x'], vertices[3]['y']\n",
    "            ], outline = color, width = 5)\n",
    "            draw.text(\n",
    "                xy = (vertices[1]['x'], vertices[1]['y']), text = f\"{prefix} {c+1}\", fill = color, anchor = 'rd', font = font\n",
    "            )\n",
    "        \n",
    "        for image in images:\n",
    "            IPython.display.display(image.resize(tuple([int(.25*x) for x in image.size])))\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.372945Z"
    }
   },
   "outputs": [],
   "source": [
    "def document_bot(question, max_output_tokens = 1000, DISTANCE = 0, MODEL = 'GEMINI', display_contexts = False, display_annotations = False, ground = True):\n",
    "    # this function directly references (without input): font\n",
    "    # DISTANCE = .1 # float in [0, 1], 0 return no additional context, 1 return all on unique pages\n",
    "    # MODEL = 'GEMINI' # one of: GEMINI, PALM_BISON, PALM_BISON32K, PALM_UNICORN\n",
    "    \n",
    "    # R: Retrival\n",
    "    if ground:\n",
    "        contexts = get_retrieval(question, DISTANCE = DISTANCE)\n",
    "    else:\n",
    "        contexts = []\n",
    "        \n",
    "    # A: Augemented\n",
    "    prompt = get_augmented(question, contexts, ground)\n",
    "    \n",
    "    # G: Generation\n",
    "    response = get_generation(prompt, max_output_tokens, MODEL)\n",
    "    \n",
    "    # Present Answer\n",
    "    get_presentation(question, contexts, DISTANCE, response, display_contexts, display_annotations)\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Out Document_Bot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1683730743606,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "X0ij5NJS8wvy",
    "outputId": "296af6b2-11ca-4be5-e4f6-1b9791ad8a57",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.376569Z"
    }
   },
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Ungrounded Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.380985Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(question, ground = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Grounded Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.385657Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Grounded Response, Print Out Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.388686Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(question, display_contexts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Grounded Response, Print Out Contexts And Annotated Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.392066Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(question, display_contexts = True, display_annotations = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Grounded Response With Expanded Contexts, Print Out Contexts And Annotated Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.396851Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(question, DISTANCE = 1, display_contexts = True, display_annotations = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Grounded Response From Out Of Context Questions\n",
    "\n",
    "Even though the question is on topic for the documents, baseball, it is about specific players and teams which is out of context of the indexed documents which are about the rules of the game of baseball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.400490Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot('Who is the best pitcher for the Dodgers?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2164,
     "status": "ok",
     "timestamp": 1683730808508,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "EsHEO-szSxxS",
    "outputId": "a4ecb9ea-33f2-4bda-8a58-32333d948198",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.404255Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(\"What is the definition of a balk?\", DISTANCE = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4565,
     "status": "ok",
     "timestamp": 1683730909879,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "DK9J55wJSxsg",
    "outputId": "f4fa9fd8-0656-411d-b51e-8ab5f5fff8aa",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.408069Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(\"Is a rule broken if three infielders are positioned on the same side of the field where the batter is more likely to hit the ball?\", DISTANCE = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2990,
     "status": "ok",
     "timestamp": 1683731403294,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "vNmawtEwJwr3",
    "outputId": "29be2d9d-079c-44b9-ff6b-f9af22dea3ab",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.411599Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(\"A batter hits a fair ball that goes over the outfield fence. Is this always a home run?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFBf_d_s9TOE",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-03-27T15:31:01.415682Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = document_bot(\"Describe the shape of bases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps!\n",
    "\n",
    "While the document bots performs well and the configuation of `DISTANCE` allows users to gather expanded contexts, it still can be improved.\n",
    "\n",
    "These enhancements will follow in additional notebooks:\n",
    "- Enhanced context retrieval with a tree of contexts\n",
    "    - This will provided a much better context for general questions that infer information from a broad section of the document. Like \"How do I play baseball?\"\n",
    "- Multi-modal prompts\n",
    "    - By included images from documents in the context retrieval and the prompt the context can also be inferred from the images.  This will help with the question about bases where the primary description in the rules is a graphic with dimensions."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(bucket.list_blobs(prefix = f'{SERIES}/{EXPERIMENT}/files_pages.json')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T21:02:06.748493Z",
     "start_time": "2024-03-28T21:02:06.412393Z"
    }
   },
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected applied-genai-v3/mlb-rules/files_pages.json\n",
      "Importing previous run from GCS\n",
      "Loaded applied-genai-v3/mlb-rules/files_pages.json\n",
      "Loaded applied-genai-v3/mlb-rules/files_pages_chunks.json\n",
      "PRIOR_PARSE: True\n"
     ]
    }
   ],
   "source": [
    "if USE_PRIOR_RUN == False:\n",
    "    PRIOR_PARSE = False\n",
    "    \n",
    "    # do a check for prior run and present message if found letting user know the prior result exists but not being used\n",
    "    if RETRIEVE_FROM == 'GCS' and len(list(bucket.list_blobs(prefix = f'{SERIES}/{EXPERIMENT}/files_pages.json'))) > 0:\n",
    "        print(f'Previous results exists in GCS but forcing the creation of new parsing with USE_PRIOR_RUN = {USE_PRIOR_RUN}')\n",
    "    elif RETRIEVE_FROM == 'BQ' and bq_table_check(f'{BQ_DATASET}.{BQ_TABLE}_files_pages'):\n",
    "        print(f'Previous results exists in BQ but forcing the creation of new parsing with USE_PRIOR_RUN = {USE_PRIOR_RUN}')\n",
    "\n",
    "elif RETRIEVE_FROM == 'GCS' and len(list(bucket.list_blobs(prefix = f'{SERIES}/{EXPERIMENT}/files_pages.json'))) > 0:\n",
    "    print(f'Detected {SERIES}/{EXPERIMENT}/files_pages.json')\n",
    "    print('Importing previous run from GCS')\n",
    "\n",
    "    # load files_pages: the file+page level information including docai responses in `parsing`\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/files_pages.json')\n",
    "    files_pages = [json.loads(line) for line in blob.download_as_text().splitlines()]\n",
    "    print(f'Loaded {SERIES}/{EXPERIMENT}/files_pages.json')\n",
    "    # load files_pages_chunks: the chunks parsed from the files+pages\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/files_pages_chunks.json')\n",
    "    files_pages_chunks = [json.loads(line) for line in blob.download_as_text().splitlines()]   \n",
    "    print(f'Loaded {SERIES}/{EXPERIMENT}/files_pages_chunks.json')\n",
    "    # Set Indicator to prevent redoing the parsing later in this notebook\n",
    "    PRIOR_PARSE = True\n",
    "    print(f'PRIOR_PARSE: {PRIOR_PARSE}')\n",
    "\n",
    "elif RETRIEVE_FROM == 'BQ' and bq_table_check(f'{BQ_DATASET}.{BQ_TABLE}_files_pages'):\n",
    "    print('Importing previous run from BigQuery')\n",
    "\n",
    "    # load files_pages: the file+page level information including docai responses in `parsing`\n",
    "    files_pages = bq.query(f'SELECT * FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}_files_pages` ORDER BY file_index, page_index').to_dataframe().to_dict('records')\n",
    "    # convert json string to dictionary:\n",
    "    for page in files_pages:\n",
    "        page['parsing'] = json.loads(page['parsing'])\n",
    "    \n",
    "    # load files_pages_chunks: the chunks parsed from the files+pages\n",
    "    files_pages_chunks = bq.query(f'SELECT * FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}_files_pages_chunks`').to_dataframe().to_dict('records')\n",
    "    #convert json string to dictionary:\n",
    "    for chunk in files_pages_chunks:\n",
    "        chunk['metadata'] = json.loads(chunk['metadata'])\n",
    "    # sort chunk by file, page, chunk number:\n",
    "    files_pages_chunks = sorted(files_pages_chunks, key = lambda x: (x['metadata']['file_index'], x['metadata']['page_index'], x['metadata']['chunk']))\n",
    "\n",
    "    # Set Indicator to prevent redoing the parsing later in this notebook\n",
    "    PRIOR_PARSE = True\n",
    "        \n",
    "else:\n",
    "    print('No previous run available to import')\n",
    "    PRIOR_PARSE = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T21:04:59.611992Z",
     "start_time": "2024-03-28T21:04:47.227708Z"
    }
   },
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOtJpcznd4BcDZJkSAIRVaj",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
